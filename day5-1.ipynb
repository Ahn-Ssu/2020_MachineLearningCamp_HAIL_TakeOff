{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"day5-1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN4PqjZgJM0vgB1hpXJdPlO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6c0838e934a146b184536279350fec9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e93f42363d2f465dba38cc8316dc9bd9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3700e4aaf1c8415d8df8da2e3ebfb867","IPY_MODEL_5299c460f42049e7bc556a02c9979e19"]}},"e93f42363d2f465dba38cc8316dc9bd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3700e4aaf1c8415d8df8da2e3ebfb867":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bf4e3316e5af4087aae426ce0bac835a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a2c3cfd22104214a52b7db8e004846b"}},"5299c460f42049e7bc556a02c9979e19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_13985328188946368c68020e69680a1f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 32867261.10it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bebda994fd8d44b9aa8ca305de3d9866"}},"bf4e3316e5af4087aae426ce0bac835a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9a2c3cfd22104214a52b7db8e004846b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"13985328188946368c68020e69680a1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bebda994fd8d44b9aa8ca305de3d9866":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5Gf2nZUwSU0i"},"source":["# 9. Deep Learning with PyTorch\n","\n","이번 세션에서는 PyTorch 패키지를 사용하여 ML모델의 정의, 학습 및 예측 실습을 합니다.\n","\n","먼저 다음의 코드를 실행하여 필요한 라이브러리들을 import 합니다.\n"]},{"cell_type":"code","metadata":{"id":"IDZKNoewSROQ"},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BWqzcCih1dW7"},"source":["## 9.1. Getting Started with PyTorch\n","\n","먼저 PyTorch이 제공하는 유용한 기능들을 unit 별로 실습해보는 시간을 갖습니다.\n","\n","(9.1. 코드 출처: https://github.com/yunjey/pytorch-tutorial) \n"]},{"cell_type":"markdown","metadata":{"id":"Q_P1isRA1mVL"},"source":["### 9.1.1. Autograd\n","\n","자동으로 미분을 해주는 기능인 autograd(automatic differentiation, AD)을 사용해 봅니다."]},{"cell_type":"code","metadata":{"id":"xvfxxkvC1sUj"},"source":["# 먼저 tensor 타입의 변수를 선언하고, 계산 그래프를 생성합니다\n","x = torch.tensor(1., requires_grad=True)\n","w = torch.tensor(2., requires_grad=True)\n","b = torch.tensor(3., requires_grad=True)\n","\n","y = w * x + b    # y = 2 * x + 3\n","\n","# backward() 함수는 계산 그래프로부터 미분값을 계산합니다.\n","y.backward()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ug-8LB7bmXws"},"source":["이제 자동으로 미분된 결과값을 출력해볼까요?"]},{"cell_type":"code","metadata":{"id":"GPuspmuUmYca","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594281291032,"user_tz":-540,"elapsed":5065,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"outputId":"5eff9516-6126-4ee9-a603-300c407b3f78"},"source":["print(x.grad)    # x.grad = 2 \n","print(w.grad)    # w.grad = 1 \n","print(b.grad)    # b.grad = 1 \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(2.)\n","tensor(1.)\n","tensor(1.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AUsqiyRz13cG"},"source":["실제 모델 학습 과정에서 autograd이 사용되는 모습은 다음과 같습니다 (다음 코드는 실습해보지 않아도 괜찮습니다)."]},{"cell_type":"code","metadata":{"id":"4GwOoAYo144T","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1594281291034,"user_tz":-540,"elapsed":5056,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"outputId":"f221def7-2d48-4b9b-febd-2392737e24a2"},"source":["# Random한 값으로 데이터를 만듭니다. x의 사이즈는 10x3; y의 사이즈는 10x2 입니다\n","x = torch.randn(10, 3)\n","y = torch.randn(10, 2)\n","\n","# x를 입력, y를 출력으로 다룰 수 있는 Linear regression model을 만듭니다\n","# (nn 표현으로는 fully connected layer를 생성합니다)\n","linear = nn.Linear(3, 2)\n","print ('w: ', linear.weight)\n","print ('b: ', linear.bias)\n","\n","# 학습에 사용될 loss function 및 optimizer를 선언 합니다\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n","\n","# Forward pass\n","pred = linear(x)\n","\n","# Compute loss\n","loss = criterion(pred, y)\n","print('** loss before 1 step optimization: ', loss.item())\n","\n","# Backward pass\n","loss.backward()\n","\n","# 계산된 미분값을 출력합니다\n","print ('dL/dw: ', linear.weight.grad) \n","print ('dL/db: ', linear.bias.grad)\n","\n","# gradient descent의 1 iteration만 실행해 봅니다.\n","optimizer.step()\n","\n","# You can also perform gradient descent at the low level\n","# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n","# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n","\n","# gradient descent의 1 iteration 후, 업데이트된 loss 값을 출력합니다.\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('** loss after 1 step optimization: ', loss.item())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w:  Parameter containing:\n","tensor([[0.1181, 0.2572, 0.1202],\n","        [0.3540, 0.3496, 0.0177]], requires_grad=True)\n","b:  Parameter containing:\n","tensor([ 0.0800, -0.1137], requires_grad=True)\n","** loss before 1 step optimization:  1.5515973567962646\n","dL/dw:  tensor([[0.3974, 0.2912, 0.5845],\n","        [0.4137, 0.3875, 0.7827]])\n","dL/db:  tensor([-0.0832,  0.2508])\n","** loss after 1 step optimization:  1.5358389616012573\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IeMlMGPf135W"},"source":["### 9.1.2. Loading data from `numpy`\n","\n","`torch.from_numpy()` 함수를 사용하면 `ndarray` 타입(`numpy`의 다차원 행렬 타입)의 데이터를 torch에서 사용하는 데이터 타입으로 (and *vice versa*) 쉽게 변환 가능합니다."]},{"cell_type":"code","metadata":{"id":"rZ5VD9rt2JNG"},"source":["import numpy as np\n","\n","# numpy array를 생성\n","x = np.array([[1, 2], [3, 4]])\n","\n","# numpy array -> torch tensor\n","y = torch.from_numpy(x)\n","\n","# torch tensor -> numpy array\n","z = y.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Kt29Jr82KLE"},"source":["### 9.1.3. DataLoader\n","\n","`DataLoader`는 학습 및 예측 과정에서 모델에 데이터를 공급해주는 역할을 합니다.\n","\n","`DataLoader`는 `sklearn.datasets`와 같이 자주 쓰이는 데이터를 내장하고 있으며, 다음 예제를 통해 CIFAR-10이라는 이미지 데이터를 불러오는 작업을 수행해볼 수 있습니다. "]},{"cell_type":"code","metadata":{"id":"dNJvJN9q2Pph","colab":{"base_uri":"https://localhost:8080/","height":119,"referenced_widgets":["6c0838e934a146b184536279350fec9a","e93f42363d2f465dba38cc8316dc9bd9","3700e4aaf1c8415d8df8da2e3ebfb867","5299c460f42049e7bc556a02c9979e19","bf4e3316e5af4087aae426ce0bac835a","9a2c3cfd22104214a52b7db8e004846b","13985328188946368c68020e69680a1f","bebda994fd8d44b9aa8ca305de3d9866"]},"executionInfo":{"status":"ok","timestamp":1594281307285,"user_tz":-540,"elapsed":21294,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"outputId":"5d39f5a4-9530-409c-86e2-b69db4ea4149"},"source":["# CIFAR-10 데이터셋을 다운로드 및 메모리에 로드\n","train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n","                                             train=True, \n","                                             transform=transforms.ToTensor(),\n","                                             download=True)\n","\n","# 데이터셋의 첫 번째 instance로 접근해보기\n","image, label = train_dataset[0]\n","print (image.size())\n","print (label)\n","\n","# 데이터셋으로부터 DataLoader 생성\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=64, \n","                                           shuffle=True)\n","\n","# DataLoader로부터 데이터를 받으며 학습하는 code skeleton\n","for images, labels in train_loader:\n","    # ----------------------------------------------\n","    # -- Your training code should be placed here --\n","    # ----------------------------------------------\n","    pass\n","\n","# Iterator를 사용하여 미니배치(mini-batch)를 구현할 수도 있음\n","# data_iter = iter(train_loader)\n","# images, labels = data_iter.next()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c0838e934a146b184536279350fec9a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../../data/cifar-10-python.tar.gz to ../../data/\n","torch.Size([3, 32, 32])\n","6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Me3bn4I2WJP"},"source":["다음의 code skeleton을 사용하면, 개별 데이터셋을 torch에서 사용되는 데이터셋 타입으로 불러들일 수 있습니다. 이렇게 작성된 dataset은 DataLoader와 함께 사용될 수 있습니다."]},{"cell_type":"code","metadata":{"id":"5rsCXsGb2YeJ","colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"status":"error","timestamp":1594281307298,"user_tz":-540,"elapsed":21303,"user":{"displayName":"Charmgil Hong","photoUrl":"","userId":"00848486677788331656"}},"outputId":"b1eaf9e4-fe68-4dd8-b04f-63f030b01fd3"},"source":["# 다음 code skeleton을 사용하여 DataLoader를 구성할 수 있습니다\n","# (TODO에 해당하는 내용을 채우기 전에는 실행되지 않습니다)\n","\n","# You should build your custom dataset as below\n","class CustomDataset(torch.utils.data.Dataset):\n","  def __init__(self):\n","    # TODO\n","    # 1. Initialize file paths or a list of file names\n","    pass\n","  def __getitem__(self, index):\n","    # TODO\n","    # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open)\n","    # 2. Preprocess the data (e.g. torchvision.Transform)\n","    # 3. Return a data pair (e.g. image and label)\n","    \n","    pass\n","  def __len__(self):\n","    # You should change 0 to the total size of your dataset\n","    return 0 \n","\n","# You can then use the prebuilt data loader\n","custom_dataset = CustomDataset()\n","train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n","                                           batch_size=64, \n","                                           shuffle=True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-1ac5d3316c48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n\u001b[1;32m     24\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                                            shuffle=True)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m---> 94\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}]},{"cell_type":"markdown","metadata":{"id":"AOZhAiJP2hYQ"},"source":["### 9.1.4. Loading a Pretrained Model\n","\n","PyTorch를 통해 작성되고, 학습된 모델(pretrained model)을 파일 형태로 주고 받을 수도 있습니다. 다음 코드는 PyTorch를 통해 제공되는 pretrained ResNet-18 (이미지 분류 모델)을 불러옵니다."]},{"cell_type":"code","metadata":{"id":"LxB1NUPj2mWl"},"source":["# Pretrained ResNet-18 다운로드 및 불러오기\n","resnet = torchvision.models.resnet18(pretrained=True)\n","\n","\n","# 다운로드 받은 pretrained model을 추가로 더 학습하는 것도 가능합니다\n","# 다음 코드는 ResNet-18의 가장 마지말 레이어만 추가 학습하는 예시를 보여줍니다\n","for param in resnet.parameters():\n","    param.requires_grad = False\n","resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example\n","\n","\n","# Forward pass를 통해 pretrained model을 예측 작업에 바로 적용해볼 수 있습니다\n","images = torch.randn(64, 3, 224, 224)\n","outputs = resnet(images)\n","print (outputs.size())     # (64, 100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xAu0RY1zyr8m"},"source":["## 9.2. Logistic Regression (in the PyTorch way!)\n","\n","이번 절에서는 우리가 이미 알고있는 logistic regression 모델을 PyTorch로 구현해보는 과정을 통해, scikit-learn (sklearn)과는 다른 PyTorch 용법을 실습해 봅니다."]},{"cell_type":"markdown","metadata":{"id":"aNlGXeNVy3-2"},"source":["실습에 사용할 데이터를 불러와 DataLoader를 구성합니다. 사용할 데이터는 손글씨 숫자 이미지를 담고있는 MNIST 데이터셋 입니다. \n","\n","![Image is not found](https://miro.medium.com/max/530/1*VAjYygFUinnygIx9eVCrQQ.png)"]},{"cell_type":"code","metadata":{"id":"OsJos21Py6PQ"},"source":["input_size = 784\n","num_classes = 10\n","batch_size = 100\n","\n","# MNIST dataset \n","train_dataset = torchvision.datasets.MNIST(root='../../data', \n","                                           train=True, \n","                                           transform=transforms.ToTensor(),  \n","                                           download=True)\n","\n","test_dataset = torchvision.datasets.MNIST(root='../../data', \n","                                          train=False, \n","                                          transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BBB2Vq9Rz2Mj"},"source":["PyTorch로 logistic regression 모델을 정의하는 과정은 다음과 같습니다."]},{"cell_type":"code","metadata":{"id":"E0kvRCEtzsIB"},"source":["# Hyper-parameter setting: 실험에 사용할 하이퍼파라메터들을 설정 합니다\n","learning_rate = 0.001\n","\n","# Model definition: 하나의 linear 레이어로 구성된 네트워크를 만듭니다\n","model = nn.Linear(input_size, num_classes)\n","\n","# Training-parameter setting: 모델이 사용할 loss function(i.e., objective function)과 optimizer를 지정합니다\n","criterion = nn.CrossEntropyLoss()  # nn.CrossEntropyLoss() computes softmax internally\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uq6lkpBHzNkp"},"source":["위 모델을 위한 학습 및 예측 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"Z-fmSgOlzPd9"},"source":["# Train the model\n","def train_logreg(train_loader, num_epochs):\n","  total_step = len(train_loader)\n","  for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","      # Reshape images to (batch_size, input_size)\n","      images = images.reshape(-1, 28*28)\n","      \n","      # Forward pass\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      \n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","      # Display the progress\n","      if (i+1) % 300 == 0:\n","        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","              .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","        \n","# Test the model\n","def test_logreg(model, test_loader):\n","  # In test phase, we don't need to compute gradients (for memory efficiency)\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","      images = images.reshape(-1, 28*28)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum()\n","\n","    # Display the result\n","    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUBETKfrzpPR"},"source":["위에서 정의한 함수들을 사용하여 logistic regression 모델을 학습/평가해 봅니다. \n","\n","**Q: 얼마나 높은 accuracy를 얻을 수 있나요?**"]},{"cell_type":"code","metadata":{"id":"jIvwcPfuu60P"},"source":["train_logreg(train_loader, num_epochs=10)\n","test_logreg(model, test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8VYcSa8yUBs"},"source":["## 9.3. Feedforward Neural Networks\n","\n","이번 절에서는 가장 기초적인 neural network이라고 할 수 있는 Feedforward Neural Network (multi-layer perceptron, MLP)을 작성하고 MNIST 데이터를 학습하는 데에 적용해보겠습니다."]},{"cell_type":"markdown","metadata":{"id":"1x4w2RrxvOOf"},"source":["먼저, 실험에 사용할 하이퍼파라메터들을 설정 합니다."]},{"cell_type":"code","metadata":{"id":"cTMN2RwEvRrv"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","hidden_size = 500\n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5PH0aZ5vEyv"},"source":["다음 class definition을 통해, 이번 절에서 사용할 FFNet 모델을 정의합니다."]},{"cell_type":"code","metadata":{"id":"rFIIFo6cvE-a"},"source":["# Fully connected neural network with one hidden layer\n","class FFNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(FFNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, num_classes)  \n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52e0kDSNw66t"},"source":["위 모델을 위한 학습 및 예측 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"S5mC1TZfw8or"},"source":["# Train the model\n","def train_ffnet(model, train_loader, num_epochs):\n","  total_step = len(train_loader)\n","  for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","      # Move tensors to the configured device\n","      \n","      images = images.reshape(-1, 28*28).to(device)\n","      labels = labels.to(device)\n","      \n","      # Forward pass\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      \n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","      # Display the progress\n","      if (i+1) % 300 == 0:\n","        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","        \n","# Test the model\n","def test_ffnet(model, test_loader):\n","  # In test phase, we don't need to compute gradients (for memory efficiency)\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","      images = images.reshape(-1, 28*28).to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","\n","    # Display the result\n","    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYJFsJgPvWxt"},"source":["위에서 정의한 모델과 함수들을 사용하여 MNIST 데이터에 적용해 봅니다. \n","\n","**Q: FFNet을 사용하여 얼마나 높은 accuracy를 얻을 수 있나요?**"]},{"cell_type":"code","metadata":{"id":"WEm48nArvYi3"},"source":["# declare a model object (instantiation)\n","model = FFNet(input_size, hidden_size, num_classes).to(device)\n","\n","# Set the loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n","\n","train_ffnet(model, train_loader, num_epochs=10)\n","test_ffnet(model, test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIMqn7690Zfa"},"source":["## 9.4. Convolutional Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"1OtUysrF0fZO"},"source":["이번 절에서는 이미지 학습/예측에 적합한 neural network의 한 종류인 Convolutional Neural Network (CNN)을 작성하고 MNIST 데이터에 적용해보겠습니다.\n","\n","가장 먼저, 실험에 사용할 하이퍼파라메터들을 설정 합니다."]},{"cell_type":"code","metadata":{"id":"wXa_HLjl45Hy"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters \n","learning_rate = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_tE26cVz6z7I"},"source":["다음 class definition을 통해, 이번 절에서 사용할 CNN 모델을 정의합니다."]},{"cell_type":"code","metadata":{"id":"rQqZcoH90g3S"},"source":["# Convolutional neural network (two convolutional layers)\n","class ConvNet(nn.Module):\n","  def __init__(self, num_classes=10):\n","    super(ConvNet, self).__init__()\n","    self.layer1 = nn.Sequential(\n","      nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n","      nn.BatchNorm2d(16),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.layer2 = nn.Sequential(\n","      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n","      nn.BatchNorm2d(32),\n","      nn.ReLU(),\n","      nn.MaxPool2d(kernel_size=2, stride=2))\n","    self.fc = nn.Linear(7*7*32, num_classes)\n","      \n","  def forward(self, x):\n","    out = self.layer1(x)\n","    out = self.layer2(out)\n","    out = out.reshape(out.size(0), -1)\n","    out = self.fc(out)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OEUrl-Fb0k3j"},"source":["위 모델을 위한 학습 및 예측 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"hTancuuf0nKQ"},"source":["# Train the model\n","def train_convnet(model, train_loader, num_epochs):\n","  total_step = len(train_loader)\n","  for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      \n","      # Forward pass\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      \n","      # Backward and optimize\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      \n","      # Display the progress\n","      if (i+1) % 300 == 0:\n","        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","              .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model      \n","def test_convnet(model, test_loader):\n","  model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","  with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      outputs = model(images)\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","\n","    # Display the result\n","    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9rWNjW5l0-Jg"},"source":["위에서 정의한 모델과 함수들을 사용하여 MNIST 데이터에 적용해 봅니다. \n","\n","**Q: CNN을 사용하여 얼마나 높은 accuracy를 얻을 수 있나요?**"]},{"cell_type":"code","metadata":{"id":"zb6KataZ09I9"},"source":["model = ConvNet(num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_convnet(model, train_loader, num_epochs=10)\n","test_convnet(model, test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6aTHxkTmcjgz"},"source":["## 9.5. Playing with a Pre-trained Network\n","\n","이번 절에서는 pre-trained model을 불러와서 다른 이미지 데이터에 적용해보고, 나아가 직접 촬영한 사진을 모델에 적용하여 올바른 예측이 이뤄지는지 살펴보도록 하겠습니다."]},{"cell_type":"markdown","metadata":{"id":"O6l8v8_ees2Z"},"source":["먼저 공유된 구글 드라이브에 연결하여 `imagenet_install`이란 이름의 디렉토리(폴더)를 각 개인의 구글 드라이브 내 `datasets/MLPracticum/` 밑으로 복사합니다.\n","\n","* 공유 드라이브: https://drive.google.com/drive/folders/14CtOIO0nIrpq-caNO50R-gu3ldjsnr4Z?usp=sharing\n","\n","복사가 진행되는 동시에 다음 코드를 실행하여 Colab 문서와 구글 드라이브를 연결합니다.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"oo8yzkKbdY9b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AuAEm4mTdiJM"},"source":["Pre-trained 모델을 쉽게 불러오기 위한 라이브러리를 설치 합니다."]},{"cell_type":"code","metadata":{"id":"LD5dhNxudxeD"},"source":["!pip install pretrainedmodels\n","!python ./drive/'My Drive'/datasets/MLPracticum/imagenet_install/setup.py install"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hQDnrqWNfpbN"},"source":["다음 주어진 코드를 입력하여 공개되어있는 pre-trained model을 개인 구글 드라이브로 다운로드 받습니다."]},{"cell_type":"code","metadata":{"id":"ocwAsbEQfy7l"},"source":["import torch\n","import pretrainedmodels\n","import pretrainedmodels.utils as utils\n","\n","model_name = 'nasnetalarge' # could be fbresnet152 or inceptionresnetv2\n","model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')\n","model.eval()\n","\n","load_img = utils.LoadImage()\n","\n","# transformations depending on the model\n","# rescale, center crop, normalize, and others (ex: ToBGR, ToRange255)\n","tf_img = utils.TransformImage(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7OMr3EOgCqK"},"source":["나중에 다시 다운로드를 받을 필요가 없도록, 다운로드 받은 모델을 개인 드라이브에 저장합니다."]},{"cell_type":"code","metadata":{"id":"8OpIRzMWgASq"},"source":["# save the model for later\n","torch.save(model, 'drive/My Drive/datasets/MLPracticum/imagenet_nasnetalarge.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOFYQQiNgcxG"},"source":["모델이 예측할 수 있는 클래스 레이블(y)의 목록을 구성합니다."]},{"cell_type":"code","metadata":{"id":"RUJyjMBYgn1m"},"source":["import csv\n","name_file = 'drive/My Drive/datasets/MLPracticum/imagenet_class_names.csv'\n","\n","imagenet_class  = {}\n","file_in = csv.reader(open(name_file))\n","for row in file_in:\n","  imagenet_class[int(row[0])] = row[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3wsz-mImic0"},"source":["#### **예측 코드 1**\n","다음의 코드는 실행될 때마다, 위에서 저장한 모델을 로드하고(`imagenet_nasnetalarge.pth`) 10행에 기술된 이미지 파일에 적용하여 해당 이미지에 담긴 내용을 판별합니다.\n","\n","즉, 10행의 이미지 파일만 바꿔주면, 여러분이 지정한 사진에 대하여 예측을 진행할 수도 있습니다."]},{"cell_type":"code","metadata":{"id":"jMbnvnzwmlRz"},"source":["try:\n","  model # does exist\n","except NameError: # model does not exist\n","  import pretrainedmodels.utils as utils\n","  model = torch.load('drive/My Drive/datasets/MLPracticum/imagenet_nasnetalarge.pth')\n","  load_img = utils.LoadImage()\n","  tf_img = utils.TransformImage(model)\n","\n","# your file name\n","img_file = './drive/My Drive/datasets/MLPracticum/test_images/001.jpg'\n","\n","input_img = load_img(img_file)\n","input_tensor = tf_img(input_img)         # 3x400x225 -> 3x299x299 size may differ\n","input_tensor = input_tensor.unsqueeze(0) # 3x299x299 -> 1x3x299x299\n","input = torch.autograd.Variable(input_tensor, requires_grad=False)\n","\n","output_logits = model(input) # 1x1000\n","\n","print(\"{} is [{}: {}]\".format(img_file ,output_logits.argmax(),\n","                           imagenet_class[int(output_logits.argmax())]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAG9J8tWncT1"},"source":["#### **예측 코드 2**\n","다음의 코드는 실행될 때마다, 위에서 저장한 모델을 로드하고(`imagenet_nasnetalarge.pth`) 10행에 기술된 디렉토리에 적용하여 해당 디렉토리 내 모든 이미지 파일에 대한 예측을 진행합니다."]},{"cell_type":"code","metadata":{"id":"BYegKZ26n0r-"},"source":["try:\n","  model # does exist\n","except NameError: # model does not exist\n","  import pretrainedmodels.utils as utils\n","  model = torch.load('drive/My Drive/datasets/MLPracticum/imagenet_nasnetalarge.pth')\n","  load_img = utils.LoadImage()\n","  tf_img = utils.TransformImage(model)\n","\n","import glob\n","dir_path = './drive/My Drive/datasets/MLPracticum/test_images/'\n","img_list = glob.glob(dir_path+'*.*')\n","\n","for img_file in img_list:\n","  input_img = load_img(img_file)\n","  input_tensor = tf_img(input_img)         # 3x400x225 -> 3x299x299 size may differ\n","  input_tensor = input_tensor.unsqueeze(0) # 3x299x299 -> 1x3x299x299\n","  input = torch.autograd.Variable(input_tensor, requires_grad=False)\n","  output_logits = model(input) # 1x1000\n","\n","  print(\"{} is [{}: {}]\".format(img_file.split('/')[-1] ,output_logits.argmax(),\n","                           imagenet_class[int(output_logits.argmax())]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vd0WRNC1oWdj"},"source":["## 과제\n","\n","1.(a) 9.2-9.4 절에서 실습을 통해 확인한 정확도를 자유 형식의 보고서에 담아주세요. 즉, PyTorch를 사용하여 구현한 LogReg, FFNet, ConvNet에서 예측 성능을 각각 제출해주기 바랍니다.\n","\n","1.(b) 각자 스마트폰에 저장되어 있는 사진 중 3장 이상을 준비하여 (구글 드라이브에 업로드하여) 9.5 절에서 실습한 pre-trained model에 적용해 봅니다. 자유 형식의 보고서를 통해 3장 이상의 사진과 pre-trained model을 사용한 예측 결과를 제출 바랍니다.\n","\n","2.다음 내용은 Day 5를 위한 준비 입니다. 원할한 실습 진행을 위해 다음 슬라이드를 참고하여 금요일 수업 전에 Twitter 개발자 어카운트를 준비해주기 바랍니다.\n","* 슬라이드: https://drive.google.com/file/d/1X8deT42sHbVwxwOLA9jj2gUbb85-XERr/view?usp=sharing"]},{"cell_type":"markdown","metadata":{"id":"xCk6W1vS36pz"},"source":["## References\n","* yunjey. PyTorch Tutorial for Deep Learning Researchers. URL: https://github.com/yunjey/pytorch-tutorial\n"]}]}